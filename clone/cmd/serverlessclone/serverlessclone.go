// Copyright 2021 Google LLC. All Rights Reserved.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// serverlessclone is a one-shot tool for downloading entries from an
// HTTP(s) exposed transparency log generated by the serverless tooling.
package main

import (
	"bytes"
	"context"
	"flag"
	"fmt"
	"io/ioutil"
	"net/http"
	"net/url"
	"os"
	"time"

	"github.com/golang/glog"
	"github.com/google/trillian-examples/clone/internal/cloner"
	"github.com/google/trillian-examples/clone/internal/verify"
	"github.com/google/trillian-examples/clone/logdb"
	"github.com/google/trillian-examples/formats/log"
	"github.com/google/trillian-examples/serverless/client"
	"github.com/transparency-dev/merkle/rfc6962"
	"golang.org/x/mod/sumdb/note"

	_ "github.com/go-sql-driver/mysql"
)

var (
	logURL   = flag.String("url", "", "The base URL for the log HTTP API, should end with a trailing slash")
	vkey     = flag.String("vkey", "", "The verification key for the log checkpoints")
	origin   = flag.String("origin", "", "The origin string for the log checkpoints")
	mysqlURI = flag.String("mysql_uri", "", "URL of a MySQL database to clone the log into. The DB should contain only one log.")

	writeBatchSize = flag.Uint("write_batch_size", 100, "The number of leaves to write in each DB transaction.")
	workers        = flag.Uint("workers", 50, "The number of worker threads to run in parallel to fetch entries.")
	timeout        = flag.Duration("timeout", 10*time.Second, "Maximum time to wait for http connections to complete.")
)

func main() {
	flag.Parse()

	if *logURL == "" {
		glog.Exit("Missing required parameter 'url'")
	}
	if *vkey == "" {
		glog.Exit("Missing required parameter 'vkey'")
	}
	if *origin == "" {
		glog.Exit("Missing required parameter 'origin'")
	}
	if *mysqlURI == "" {
		glog.Exit("Missing required parameter 'mysql_uri'")
	}

	ctx := context.Background()
	db, err := logdb.NewDatabase(*mysqlURI)
	if err != nil {
		glog.Exitf("Failed to connect to database: %q", err)
	}

	v, err := note.NewVerifier(*vkey)
	if err != nil {
		glog.Exitf("Failed to create verifier: %v", err)
	}
	u, err := url.Parse(*logURL)
	if err != nil {
		glog.Exitf("Invalid log URL %q: %v", *logURL, err)
	}
	f := newFetcher(u)

	targetCp, rawCp, err := client.FetchCheckpoint(ctx, f, v, *origin)
	if err != nil {
		glog.Exitf("Failed to get latest checkpoint from log: %v", err)
	}
	glog.Infof("Target checkpoint is for tree size %d", targetCp.Size)

	if err := clone(ctx, db, f, targetCp); err != nil {
		glog.Exitf("Failed to clone: %v", err)
	}

	// Verify the downloaded leaves with the target checkpoint, and if it verifies, persist the checkpoint.
	h := rfc6962.DefaultHasher
	lh := func(_ uint64, preimage []byte) []byte {
		return h.HashLeaf(preimage)
	}

	lv := verify.NewLogVerifier(db, lh, h.HashChildren)
	root, crs, err := lv.MerkleRoot(ctx, uint64(targetCp.Size))
	if err != nil {
		glog.Exitf("Failed to compute root: %q", err)
	}
	if !bytes.Equal(targetCp.Hash[:], root) {
		glog.Exitf("Computed root %x != provided checkpoint %x for tree size %d", root, targetCp.Hash, targetCp.Size)
	}
	glog.Infof("Got matching roots for tree size %d: %x", targetCp.Size, root)
	if err := db.WriteCheckpoint(ctx, uint64(targetCp.Size), rawCp, crs); err != nil {
		glog.Exitf("Failed to update database with new checkpoint: %v", err)
	}
}

func clone(ctx context.Context, db *logdb.Database, f client.Fetcher, targetCp *log.Checkpoint) error {
	cl := cloner.New(*workers, 1, *writeBatchSize, db)

	next, err := cl.Next()
	if err != nil {
		return fmt.Errorf("couldn't determine first leaf to fetch: %v", err)
	}
	if next >= uint64(targetCp.Size) {
		glog.Infof("No work to do. Local tree size = %d, latest log tree size = %d", next, targetCp.Size)
		return nil
	}

	batchFetch := func(start uint64, leaves [][]byte) error {
		if len(leaves) != 1 {
			return fmt.Errorf("true batch fetching not supported")
		}
		leaf, err := client.GetLeaf(ctx, f, start)
		leaves[0] = leaf
		return err
	}

	if err := cl.Clone(ctx, uint64(targetCp.Size), batchFetch); err != nil {
		return fmt.Errorf("failed to clone log: %v", err)
	}
	return nil
}

// newFetcher creates a Fetcher for the log at the given root location.
func newFetcher(root *url.URL) client.Fetcher {
	get := getByScheme[root.Scheme]
	if get == nil {
		panic(fmt.Errorf("unsupported URL scheme %s", root.Scheme))
	}

	return func(ctx context.Context, p string) ([]byte, error) {
		u, err := root.Parse(p)
		if err != nil {
			return nil, err
		}
		return get(ctx, u)
	}
}

var getByScheme = map[string]func(context.Context, *url.URL) ([]byte, error){
	"http":  readHTTP,
	"https": readHTTP,
}

func readHTTP(ctx context.Context, u *url.URL) ([]byte, error) {
	req, err := http.NewRequest("GET", u.String(), nil)
	if err != nil {
		return nil, err
	}
	c := http.Client{
		Timeout: *timeout,
	}
	resp, err := c.Do(req.WithContext(ctx))
	if err != nil {
		return nil, err
	}
	switch resp.StatusCode {
	case 404:
		glog.Infof("Not found: %q", u.String())
		return nil, os.ErrNotExist
	case 200:
		break
	default:
		return nil, fmt.Errorf("unexpected http status %q", resp.Status)
	}
	defer resp.Body.Close()
	return ioutil.ReadAll(resp.Body)
}
